{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters\n",
    "      b (scalar):             model parameter\n",
    "\n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b\n",
    "    return p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters\n",
    "      b (scalar)       : model parameter\n",
    "\n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        f = predict(X[i], w, b)\n",
    "        cost += (f - y[i]) ** 2\n",
    "    cost /= (2 * m)\n",
    "    return cost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def gradient(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters\n",
    "      b (scalar)       : model parameter\n",
    "\n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros((n, ))\n",
    "    dj_db = 0\n",
    "    for i in range(m):\n",
    "        err = (predict(X[i], w, b) - y[i])\n",
    "        for j in range(n):\n",
    "            dj_dw += err * X[i, j]\n",
    "        dj_db += err\n",
    "\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "\n",
    "    return dj_dw, dj_db"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_func, gradient_func, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters\n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_func       : function to compute cost\n",
    "      gradient_func   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "\n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters\n",
    "      b (scalar)       : Updated value of parameter\n",
    "      \"\"\"\n",
    "    J_hist = []\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        dj_dw, dj_db = gradient_func(X, y, w, b)\n",
    "        w -= alpha * dj_dw\n",
    "        b -= alpha * dj_db\n",
    "\n",
    "        if i < 100000:\n",
    "            J_hist.append(cost_func(X, y, w, b))\n",
    "\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_hist[-1]:8.2f}   \")\n",
    "\n",
    "        return w, b, J_hist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  4073.16   \n",
      "b,w found by gradient descent: 0.00,[0.248112 0.248112 0.248112 0.248112] \n",
      "prediction: 534.68, target value: 460\n",
      "prediction: 362.49, target value: 232\n",
      "prediction: 220.82, target value: 178\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "\n",
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b, cost, gradient, alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.  4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5.  5.1 5.2 5.3 5.4 5.5 5.6 5.7\n",
      " 5.8 5.9 6.  6.1 6.2 6.3 6.4 6.5 6.6 6.7 6.8 6.9 7.  7.1 7.2 7.3 7.4 7.5\n",
      " 7.6 7.7 7.8 7.9 8.  8.1 8.2 8.3 8.4 8.5 8.6 8.7 8.8 8.9 9.  9.1 9.2 9.3\n",
      " 9.4 9.5 9.6 9.7 9.8 9.9]\n",
      "[0.30540693 0.63981301 0.09650967 0.48017494 0.16558147 0.96101865\n",
      " 0.69147737 0.82936124 0.09019627 0.39290061]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(4, 10, 0.1)\n",
    "print(f'{a}')\n",
    "b = np.random.rand(10)\n",
    "print(f'{b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def z_score_normalization(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "\n",
    "    normalized = (X - mean) / sigma\n",
    "    return normalized, mean, sigma"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[ 1.26311506,  1.33630621, -0.70710678,  1.22474487],\n        [-0.08073519, -0.26726124,  1.41421356,  0.        ],\n        [-1.18237987, -1.06904497, -0.70710678, -1.22474487]]),\n array([1.45733333e+03, 3.33333333e+00, 1.33333333e+00, 4.00000000e+01]),\n array([5.11961804e+02, 1.24721913e+00, 4.71404521e-01, 4.08248290e+00]))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_score_normalization(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scikit-Learn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak to Peak range by column in Raw        X:[1252    3    1   10]\n",
      "Peak to Peak range by column in Normalized X:[2.44549494 2.40535118 2.12132034 2.44948974]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X_train)\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")\n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor()\n",
      "number of iterations completed: 6, number of weight updates: 19.0\n"
     ]
    }
   ],
   "source": [
    "sgdr = SGDRegressor(max_iter=1000)\n",
    "sgdr.fit(X_train, y_train)\n",
    "print(sgdr)\n",
    "print(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters:                   w: [-6.04768198e+11  3.37882112e+08 -8.62889125e+09 -1.20429037e+11], b:[-4.01900938e+09]\n"
     ]
    }
   ],
   "source": [
    "b = sgdr.intercept_\n",
    "w = sgdr.coef_\n",
    "print(f\"model parameters:                   w: {w}, b:{b}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-9.08852613e+11,  3.25136679e+10,  8.64281917e+11])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgdr.predict(X_norm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}